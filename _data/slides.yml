- title: YAHOO
  desc: >
    <p>does this work?<br/>
    This is an open-source slideshow, you can <a href="https://github.com/alberto56/dcycle_3_day_training/edit/gh-pages/_data/slides.yml">fork or edit it on GitHub</a></p>.

- title: YAHOO
  desc: >
    does this work?

- title: YAHOO
  desc: >
    does this work?


Introduction
------------

This three-day session will focus on getting the Drupal development team to agree on best practices for development. The idea is to decide *as a team* what is best for us, so all of us feel that the methods decided are *owned by the team* and not imposed by anyone.

Approach
--------

 * A gradual and incremental approach is proposed (like Agile itself)
 * Human-centered: recognize that our interactions, not the tools, will make our procedures valuable.
 * Simplicity: our agreed-upon core set of procedures should be easily understandable.
 * Integrating deployment, testing, and ci, they all go together
 * Hands-on: github, real server. A real project will be deployed to GitHub, and we will have a working Jenkins instance at our disposal.
 * Extensibility: even if we decide on somewhat simple core processes, this should not limit us from doing more complex things outside of our common approach. For example, if we decide that in our common approach we have two git branches, we should not be prevented from using dozens of branches between developers if we feel that is useful.
 * Inviolability (or "Don't subvert the process"): we should all agree that once we decide *as a team* to do things a certain way, we should never subvert that process. For example, if we decide that Jenkins should update dev, then no developer should ever update dev directly.
 * Writing tests depends on a solid, standardized development procedure. That is why we need to figure out how we use CI, how we do deployment (both initial and incremental), and when we clone the database. Once we decide on that, we can look at testing.

Objectives
----------

Our main objective *Quality*. Testing, CI, deployment, Agile: all of these are tools to achieve quality. In my experience, it is important to agree on an approach to the the following aspects of Drupal development.

 * Software development methodoloy: Do we use Agile? how do we do it?
 * How do we do an initial deployment (to dev, preprod, to a new developer's laptop)?
 * How do we do an incremental deployment? Is the process standardized?
 * How do we push to dev? How? Jenkins?
 * Do we use a preproduction environment? How do we push to it?
 * In which contexts is it OK to clone the database?
 * How do we peer review? How does it fit into our process?
 * Writing automated tests should come after we figured out all the rest, that's why we'll talk about on day 2.
 * Which metrics do we track over time? Do we define thresholds above/below which our build should fail? For example, how many minor styling errors (using the coder module) do we tolerate? 100? 100000? no limit? What is our objective for test coverage of custom code? No lower limit? 10%? 40%?
 * How do we use server config management and virtualization tools like Puppet, Ansible, Docker, Vagrant...

How we deploy initially
-----------------------

I will present a fully-functional dummy site with a working production environment, and how we can use a one-step [site deployment module](http://dcycleproject.org/blog/44/what-site-deployment-module) [without cloning the database](http://dcycleproject.org/blog/48/do-not-clone-database), using instead [realistic rich dummy content](https://www.drupal.org/project/realistic_dummy_content) to have a working development copy.

We will look at:

 * the reasoning behind not cloning the database.
 * the concept of a "known-good starting point" introduced by Jez Humble
 * some edge cases which might cause problems if we do not clone the database, for example [taxonomy incremental ids in views code generated by features](http://dcycleproject.org/blog/50/do-not-use-incremental-ids-your-code).

The following approach has worked for me on small to large projects: deploying new environments goes something like this:

    drush site-install -y
    drush en mysite_deploy -y

In development environments we can add some development tools as well as dummy content

    drush en mysite_devel -y
    drush generate-realistic -y

How we deploy incrementally
---------------------------

We should strive for an approach where deploying incrementally between any two versions of a code, on any environment, should be done in a standardized manner. This is key to having deployments done by a ci server, and for staff to get an up-to-date version of the site on their laptop in minutes. I have used some version of the following script on several projects without any glitch:

  drush vset maintenance_mode 1
    git pull origin master # or whatever branch we want, prod for production for example.
    drush rr # rebuild the registry
    drush updb -y # run update hooks
    drush fra # revert all features
    drush cc all
    drush cron
  drush vset maintenance_mode 0

What does our CI server do?
---------------------------

Now that we are limiting our use of unversioned database cloning, and we a reliable reproducible initial and incremental deployment technique, we can set up our CI server to deploy for us. We can even [continuously deploy](http://dcycleproject.org/blog/46/continuous-deployment-drupal-style) to preproduction (or even to production if we are brazen enough) if we so choose.

Tracking metrics and thresholds
-------------------------------

Our CI server can do anything we can do on the command line, not just deploy. We will set up the following:

 * Run our first test (we will use a unit test, database test or request-response test).
 * Run code quality metrics, track it and set thresholds
 * Save artifacts for future use

Branches and code review: how we do it
-------------------------

On the projects I've set up in the past:

 * any team member could push to master.
 * jenkins then sets the code to red (if tests fail) or green
 * if tests pass for a commit, it is automatically moved to a prod branch and the common dev environment is updated.
 * the incremental deployment script is launched by a human being on the production server at the end of every sprint demo.

This has the advantage of being simple, but the drawback of not imposing code review. We need to figure out some techniques and tools to impose code review if we so desire. Phabricator, Gerrit, Branches can be discussed.

Legacy sites: what they are and how to deal with them
-----------------------------------------------------

We can define together what is a legacy site. According to Michael Feathers in the book "Working Effectively with Legacy Code", legacy code is code which is not tested.

I would suggest the following definitions, to be discussed:

 * a legacy project is a project which is not under active continuous integration.
 * a project which has zero tests is a legacy project.
 * a project with more than zero tests is not a legacy project.
 * any line of code which not covered by tests is a legacy line of code.
 * test coverage for a project is a measure of its quality. (what percentage of lines of code are covered by tests).

Furthermore, anything which is not *deployable* is not really code at all but data. For example, any nodes, taxonomy terms, views which depend on taxonomy term ids (or other incremental ids)...

Let's imagine a Drupal project which has no tests and is not covered by an active CI job. How do we deal with this? I would suggest an iterative approach:

 * For every bug report or feature request, make sure new code is tested, deployable, and checked by a CI job.
 * Everything outside of our new modern code should be considered either data (if it's in the database or otherwise unversioned -- the files directory, etc.), or legacy code (if it's in git, but somehow untestable, either because there are no tests for it, or it contains incremental IDs...)

In my mind the barrier to entry to transform a legacy project to a modern project is very low: simple write one test and put the entire project under a ci job. At first our project will have very low test coverage, a high percentage of data vs code. Our goal should be to increase the *quality* of this project over time, not in one fell swoop.

Part 2: tests
-------------

### The test pyramid

Mike Cohn has proposed the concept of a [test pyramid](http://martinfowler.com/bliki/TestPyramid.html). In my experience is it crucial to follow this principle: unit tests first, then database tests, then rich gui tests.

### Unit tests: cheapest

We will introduce Simpletest and PHPUnit for unit tests.

### Database tests: relatively cheap (if done right)

Database tests ignore the current database, and run a new deployment of the site to test it, this is why in my experience it is important to have a one-step deployment process without cloning the database. We'll give one example and introduce ways to make the tests faster (slow tests are irritating)..

### Mocking: how and why?

We'll introduce how to deal with third-party systems by abstracting the interface between our site and the third-party service using mocking. We'll also look at some of the advanced mocking capabilities in PHPUnit, which ships with D8.

### Rich UI and browser-specific tests: expensive

We'll look at running tests on different browsers using Selenium, and we'll look at how a headless webkit-based browser (PhantomJS) can help test rich dynamic UI.

### Behat: why/how to use?

Finally we'll look at what Behat is, what it's designed for, and in which contexts we will want to use it.

Part 3: Metrics
---------------

Jenkins can track different metrics, set thresholds, and create nice graphs. We can track things like:

 * Code quality (pdepend, coder...)
 * Test coverage.
 * Css quality
 * a11y (accessibility) tests
 * whatever else you can do in the command line...

Resources
---------

 * [The Dcycle blog and manifesto](http://dcycleproject.org)
 * Book "Continuous delivery" by Jez Humble
 * Book "Working Effectively with Legacy Code" by Michael Feathers
